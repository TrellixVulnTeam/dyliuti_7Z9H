**“纸上得来终觉浅，绝知此事要躬行。”  —— 陆游**

**"Practice，practice，practice and summary makes perfect" —— dyliuti**

------



**马尔可夫：**

1.马尔可夫链的观测序列的概率求的是联合概率分布，具体到语言模型，该联合概率就对应句子的可能性。

2.通过简化状态依赖关系（随机变量非独立），形成一阶/二阶马尔可夫链。

3.用隐马尔可夫概念来套，就是状态可见，观测序列等同于状态序列，两者之间是等价关系，不是随机过程。

4.马尔可夫的计算简单，计数得频率，再求极大似然

**隐马尔可夫：**

问题1：给定观测序列、状态转移矩阵和概率分布矩阵的情况下，求观测序列的概率（概率计算问题）。

问题2：给定观测序列、状态转移矩阵和概率分布矩阵的情况下，求最可能的状态序列，最好解释观测序列（观测序列已知，哪个状态序列概率最大）。需要路径记忆。（预测问题，也是解码问题）

问题3：给定观测序列的情况下，用极大似然法求状态转移矩阵和概率分布矩阵，使观测序列在这两矩阵下概率最大。（学习问题）

<br>

**总结：**

对于马儿可夫可以直接用极大似然估计法或贝叶斯估计模型参数。

对于隐马儿可夫的三个问题：动态规划贯穿全部（前向、后向、viterbi算法）。

问题1：概率问题。直接用前向算法即可。

问题2：预测问题。维特比算法求得，其实就是求最优路径。最优是动态规划的范畴，路径是记忆缓存的范畴。

问题3：学习问题。利用前向、后向算法求得期望。再利用期望对模型参数进行极大似然估计（Baum-Welch算法）。

有意思的是，给定观测序列，相当于是给定了label。使观测序列在两者的前提下概率最大，即可调节状态转移矩阵和概率分布矩阵（相当于神经网络中前向传递的权重）。可以将状态转移公式，转换为矩阵形式点积与element-wise的形式。目标是求最大概率P，定义损失函数-log(P)，转换为损失函数最小化问题，即可用梯度下降的数值方法求得最优状态与状态转移矩阵。（梯度下降近似法）

<br>

**文件说明：**

一、1st_sites.py与2st_frost.py是马尔可夫模型的应用。

二、generate.py：利用状态转移分布和观测概率分布随机生成观测序列。

三、hmm_discrete前缀的是解决问题3的具体实现。tf对应梯度下降近似发。np对应常规Baum-Welch算法。还有对应问题2的viterbi算法。

四、hmm_speech_tag.py：将HMM用于序列标注（分类问题）。

<br>

**数据集下载：**

[Markov数据集下载，解压后将Markov文件夹放在Data文件夹下](https://drive.google.com/file/d/1G3rmYtY7Io754vVogcEdtskvTqYfsiuF/view?usp=sharing)

